{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5a7703d",
   "metadata": {},
   "source": [
    "# Magistritöö : BERT mudeli kohandamine eesti keelele\n",
    "\n",
    "## Taust\n",
    "\n",
    "Sisendtekst -> **tokenizer** -> **embedding** -> transformer\n",
    "\n",
    "Tavalises BERTis hakitakse sõna tokenizeris osadeks (täissõna või n-gram) ning sellest tehakse 3-komponendiline embedding (token embedding, segment embedding ja sequence embedding).\n",
    "\n",
    "* Token embedding - tokeni vektor \n",
    "* Segment embedding - 1/0 vektor paarissisendite eristamiseks\n",
    "* Sequence embedding - vektor, mis tähistab positsiooni sisendtekstis\n",
    "\n",
    "Embeddingud liidetakse ja saadakse sisendembedding transformerile.  \n",
    "Vaata siit:\n",
    "https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a\n",
    "\n",
    "\n",
    "## Ülesanne\n",
    "\n",
    "Anda sisendiga kaasa rohkem infot, kasutades estnltk vahendeid (morfoloogia osad). Selleks on vaja:\n",
    "\n",
    "1) Muuta tokenizerit, et saada tokeni asemel lemma ja vorm  \n",
    "2) Muuta embeddinguid, tokem embeddingu asemel leida lemma embedding ja vormi embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff58f3b",
   "metadata": {},
   "source": [
    "## 1) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9dd8b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 572, 3611, 11838, 1709, 49892, 229, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tavaline BERT tokenizer\n",
    "\n",
    "from transformers import BertTokenizer, PreTrainedTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tartuNLP/EstBERT\")\n",
    "\n",
    "tokenizer(\"Kuidas kirjutada magistritööd?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ed41656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#import os\n",
    "#current = os.path.dirname(os.path.realpath(\"test.ipnyb\"))\n",
    "#parent = os.path.dirname(current)\n",
    "#sys.path.append(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2667998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tallinna', ''), ('linn', 'sg n'), ('algatama', 'b'), ('paldiski', ''), ('maantee', 'sg g'), ('ääres', ''), ('hotell', 'sg n'), ('tallinna', ''), ('kõrval', ''), ('asuv', 'sg g'), ('suur', 'sg g'), ('vundamendiauk', 'sg g'), ('ja', ''), ('tühermaa', 'sg g'), ('detailplaneering', 'sg g'), ('koostamine', 'sg g'), (',', ''), ('ehitustöö', 'pl g'), ('alustamine', 'sg p'), ('takistama', 'vad'), ('aga', ''), ('ala', 'sg g'), ('segane', 'pl n'), ('omandisuhe', 'pl n'), ('.', '')]\n",
      "\n",
      "{'input_ids': [(2, 2), (32476, 1), (2733, 34), (1, 53), (1, 1), (3937, 30), (4462, 1), (12535, 34), (32476, 1), (1773, 1), (8285, 30), (502, 30), (1, 30), (37, 1), (1, 30), (1, 30), (14457, 30), (11, 1), (25063, 45), (35501, 35), (1, 108), (179, 1), (440, 30), (18569, 49), (1, 49), (15, 1), (3, 3)], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [[1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# Uus tokeniseerija\n",
    "\n",
    "from src.transformers.models.bert.tokenization_bert import BertTokenizer\n",
    "tokenizer = BertTokenizer(vocab_file = \"vocab.txt\", vocab_file_form = \"vocab_form.txt\")\n",
    "input_tekst = \"Maril on paha tuju, aga see on fine.\"\n",
    "input_tekst = \"Tallinna linn algatab Paldiski maantee ääres Hotell Tallinna kõrval asuva suure vundamendiaugu ja tühermaa detailplaneeringu koostamise, ehitustööde alustamist takistavad aga ala segased omandisuhted.\"\n",
    "\n",
    "print(tokenizer.tokenize(input_tekst))\n",
    "print(\"\")\n",
    "print(tokenizer(input_tekst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1871a727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='', vocab_size=50004, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66119c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb2b37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[    2,     2],\n",
      "         [32476,     1],\n",
      "         [ 2733,    34],\n",
      "         [    1,    53],\n",
      "         [    1,     1],\n",
      "         [ 3937,    30],\n",
      "         [ 4462,     1],\n",
      "         [12535,    34],\n",
      "         [32476,     1],\n",
      "         [ 1773,     1],\n",
      "         [ 8285,    30],\n",
      "         [  502,    30],\n",
      "         [    1,    30],\n",
      "         [   37,     1],\n",
      "         [    1,    30],\n",
      "         [    1,    30],\n",
      "         [14457,    30],\n",
      "         [   11,     1],\n",
      "         [25063,    45],\n",
      "         [35501,    35],\n",
      "         [    1,   108],\n",
      "         [  179,     1],\n",
      "         [  440,    30],\n",
      "         [18569,    49],\n",
      "         [    1,    49],\n",
      "         [   15,     1],\n",
      "         [    3,     3]]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tokenizer_output = tokenizer(input_tekst)\n",
    "words_tensor = torch.tensor([tokenizer_output[\"input_ids\"]])\n",
    "segments_tensor = torch.tensor([tokenizer_output[\"token_type_ids\"]])\n",
    "print(words_tensor)\n",
    "print(segments_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbcf014",
   "metadata": {},
   "source": [
    "## 2) Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bb90748",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tartuNLP/EstBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'bert.embeddings.word_embeddings.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at tartuNLP/EstBERT and are newly initialized: ['bert.embeddings.lemma_embeddings.weight', 'bert.embeddings.form_embeddings.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from src.transformers.models.bert.modeling_bert import BertEmbeddings, BertModel\n",
    "from src.transformers.models.bert.configuration_bert import BertConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "config = BertConfig()\n",
    "embedding = BertEmbeddings(config)\n",
    "model = BertModel(config).from_pretrained(\"tartuNLP/EstBERT\")\n",
    "\n",
    "# model = BertModel(config) \n",
    "# Annab index out of range errorit, kuna tokeniseerija kasutab EstBERT vocab.txt faili\n",
    "# Kui sõnal pole vormi, siis hetkel on vasteks tühi sõne -> ID 49881\n",
    "# Tavalisel BERTil sõnastiku suurus ~30 000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd332d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.7084, -0.1104, -0.5107,  ..., -0.5266,  0.0329, -0.3554],\n",
      "         [ 1.1890, -0.2122, -0.4336,  ..., -0.6041,  0.1565,  1.0217],\n",
      "         [ 0.6978, -0.3808, -0.9276,  ..., -0.6450,  0.0800,  0.5463],\n",
      "         ...,\n",
      "         [ 0.1490, -0.2475, -0.2370,  ..., -0.8311,  0.3278,  0.0696],\n",
      "         [ 0.7233, -0.1713, -0.2095,  ..., -0.7554, -0.0536,  0.2238],\n",
      "         [ 0.5969, -0.1337, -0.3339,  ..., -1.1777,  0.1442,  0.5726]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7575, -0.1106, -0.6539, -0.0478,  0.2228,  0.1760,  0.5432,  0.3282,\n",
      "         -0.3785,  0.6589,  0.5764,  0.3764,  0.1170,  0.5791,  0.0266,  0.6563,\n",
      "          0.5073, -0.5673,  0.1771,  0.4460,  0.3728, -0.2305,  0.2115,  0.3274,\n",
      "          0.3929, -0.4446, -0.3809,  0.2946,  0.8287, -0.5622,  0.4769,  0.2044,\n",
      "         -0.2921, -0.2330,  0.3328,  0.1427,  0.4485, -0.3210, -0.3539,  0.1338,\n",
      "         -0.1099,  0.4051, -0.7250, -0.0788, -0.2713,  0.4299, -0.6499, -0.1185,\n",
      "          0.0653, -0.0037, -0.6304, -0.5463, -0.1479,  0.6570, -0.1014, -0.2047,\n",
      "         -0.0195, -0.5432, -0.3330, -0.1524, -0.5668,  0.4309, -0.6997,  0.2589,\n",
      "          0.1910, -0.1105,  0.8590, -0.4132,  0.5281,  0.4450,  0.3048,  0.1349,\n",
      "          0.0048,  0.0128,  0.1273, -0.6064, -0.3319,  0.2413, -0.0117, -0.0149,\n",
      "          0.3868, -0.2367, -0.0591, -0.1583,  0.7599, -0.1732,  0.0956,  0.2564,\n",
      "          0.4461, -0.5332,  0.3655, -0.4360, -0.0516, -0.8128,  0.3605, -0.3962,\n",
      "         -0.8490,  0.4290,  0.3536,  0.3385, -0.4775,  0.5407,  0.5303, -0.6911,\n",
      "          0.1790, -0.4857,  0.3548, -0.5518, -0.4419,  0.2678,  0.0335,  0.2652,\n",
      "          0.4266, -0.6913,  0.4977, -0.3143, -0.2173, -0.1986, -0.2403,  0.3309,\n",
      "          0.4303,  0.2816, -0.2858,  0.5062, -0.7044, -0.1105,  0.1648,  0.1465,\n",
      "         -0.2457,  0.3360, -0.7339, -0.4623, -0.3000, -0.3524,  0.3421, -0.0453,\n",
      "          0.0641, -0.2272,  0.7740, -0.4732,  0.1317, -0.5453, -0.2146,  0.0786,\n",
      "          0.2835,  0.0081,  0.1784, -0.0317,  0.3876, -0.2217,  0.7119,  0.0898,\n",
      "          0.0767, -0.6323,  0.0714, -0.5509, -0.4224, -0.1827,  0.5370, -0.5304,\n",
      "          0.5556,  0.3489, -0.5640, -0.2716,  0.7089,  0.1276, -0.3748,  0.2902,\n",
      "         -0.4328, -0.6175, -0.3652, -0.2475,  0.3144,  0.5266, -0.2512, -0.0681,\n",
      "         -0.1257,  0.3563,  0.2596, -0.6649, -0.5732, -0.1194, -0.1130, -0.0017,\n",
      "         -0.4057, -0.1370,  0.5546, -0.2334,  0.5431,  0.0104, -0.4084,  0.1216,\n",
      "          0.4384, -0.4104,  0.0838, -0.4135,  0.5104,  0.3791,  0.2729, -0.4681,\n",
      "         -0.5591, -0.7752, -0.3792,  0.1115, -0.2677, -0.0084,  0.6933,  0.2167,\n",
      "          0.4628,  0.2890, -0.7598,  0.1977, -0.0352,  0.4657,  0.6410, -0.2760,\n",
      "         -0.5567,  0.0021, -0.4631, -0.2790, -0.0684, -0.2529, -0.2858, -0.2059,\n",
      "          0.4091, -0.5150, -0.2101, -0.2678,  0.2983, -0.5609,  0.2547, -0.8757,\n",
      "         -0.1658,  0.1123,  0.1274, -0.0896,  0.0931, -0.2084, -0.3419,  0.1207,\n",
      "          0.2438,  0.6638, -0.4690,  0.1589,  0.0434,  0.6125,  0.6455,  0.7355,\n",
      "         -0.3628,  0.4180, -0.1302,  0.2464, -0.0179,  0.0082, -0.0518, -0.6621,\n",
      "         -0.6844, -0.6076, -0.2110, -0.0446,  0.1738,  0.5271, -0.0628,  0.3959,\n",
      "          0.1007, -0.5246, -0.1679,  0.4323,  0.4673,  0.0421, -0.0715,  0.1419,\n",
      "         -0.0482, -0.0235, -0.3183, -0.3340,  0.1759,  0.3338, -0.4323,  0.3060,\n",
      "          0.0767, -0.7129,  0.6227, -0.1781,  0.3501, -0.2843,  0.1673, -0.1374,\n",
      "          0.2945,  0.6482,  0.3291,  0.3335, -0.1183, -0.5617,  0.0932, -0.6011,\n",
      "          0.2281,  0.3260, -0.0708, -0.2503, -0.0181, -0.3184, -0.1564,  0.4292,\n",
      "          0.8723,  0.3506,  0.6828,  0.2288,  0.5063,  0.2112,  0.0247,  0.3753,\n",
      "          0.3852, -0.0425, -0.2582,  0.3422, -0.8997, -0.0771,  0.2613,  0.1656,\n",
      "         -0.4804, -0.6196,  0.2531,  0.2255,  0.4632,  0.1281, -0.2944,  0.0048,\n",
      "          0.2541,  0.4657,  0.1718,  0.1419, -0.2890, -0.2655, -0.1688, -0.5871,\n",
      "          0.3675, -0.6023,  0.5825, -0.5335, -0.2622, -0.2318,  0.2089, -0.1168,\n",
      "         -0.7413, -0.3086,  0.0557,  0.4071, -0.2993,  0.2282, -0.0292, -0.1561,\n",
      "         -0.0344, -0.1876,  0.0099, -0.0318, -0.1275,  0.4653,  0.1819,  0.5307,\n",
      "         -0.3426, -0.5989,  0.0897,  0.3045, -0.4071, -0.5080,  0.0698,  0.0351,\n",
      "          0.2299, -0.4767,  0.2387,  0.4868,  0.5590,  0.3338,  0.4082,  0.6042,\n",
      "         -0.7037,  0.0031,  0.2313, -0.1291, -0.3424, -0.2955,  0.2898,  0.6898,\n",
      "          0.4301, -0.6599,  0.4637, -0.1991,  0.3670,  0.4862, -0.9069, -0.7724,\n",
      "          0.4296,  0.2834, -0.7926, -0.3535, -0.5081,  0.5614,  0.6796, -0.4313,\n",
      "         -0.6418, -0.2539,  0.4134,  0.4485,  0.2357,  0.8370,  0.2869,  0.1665,\n",
      "          0.3872,  0.2363,  0.0395, -0.4472, -0.0827, -0.0323,  0.8313, -0.3644,\n",
      "          0.5807,  0.2275, -0.6907, -0.3878, -0.4812, -0.3302,  0.3455,  0.2199,\n",
      "          0.0736, -0.1938,  0.2598,  0.0620,  0.7373,  0.0068,  0.2078, -0.0486,\n",
      "         -0.1565,  0.0430,  0.2108, -0.3765,  0.8127, -0.2014,  0.0252,  0.6049,\n",
      "         -0.1541, -0.1137,  0.0599,  0.1634, -0.5843,  0.0239, -0.5009, -0.1448,\n",
      "         -0.0461, -0.5699, -0.1767, -0.0060,  0.1927, -0.8716,  0.4303,  0.0735,\n",
      "          0.5426,  0.2292, -0.5500, -0.6445, -0.0622, -0.6059,  0.3980,  0.0385,\n",
      "         -0.0290,  0.1000, -0.4435, -0.3297, -0.1495, -0.4004, -0.2433,  0.2335,\n",
      "         -0.4105, -0.2191,  0.2389, -0.1030, -0.2102, -0.1885, -0.2789, -0.2757,\n",
      "          0.2900, -0.2470, -0.1305, -0.3738,  0.3112, -0.0823, -0.5931,  0.2640,\n",
      "         -0.4903,  0.7580, -0.5824,  0.0873, -0.5548,  0.4907,  0.2788,  0.1989,\n",
      "         -0.5564, -0.7460, -0.2175, -0.0954, -0.8134,  0.0795,  0.1099,  0.6367,\n",
      "         -0.5412, -0.0495,  0.1107,  0.5085,  0.6329,  0.6693, -0.2642, -0.3835,\n",
      "          0.8044,  0.1377,  0.4512,  0.5698,  0.2555, -0.4133, -0.5292,  0.4593,\n",
      "         -0.5055,  0.6169, -0.0239, -0.2289,  0.3231, -0.0193, -0.5690,  0.0938,\n",
      "         -0.3688, -0.0981,  0.2645, -0.6441,  0.1017, -0.0336, -0.2076,  0.6037,\n",
      "          0.1927,  0.4991,  0.3721, -0.6163, -0.1606,  0.6042,  0.3511,  0.5433,\n",
      "         -0.2979, -0.5464,  0.5609,  0.6169, -0.8752, -0.5694, -0.2458,  0.0606,\n",
      "          0.1401, -0.4913,  0.2020, -0.3403,  0.5265, -0.4669, -0.0570,  0.0332,\n",
      "          0.1263,  0.0052,  0.5459, -0.0923,  0.0988,  0.4034, -0.0738,  0.7119,\n",
      "         -0.5343, -0.4352,  0.0637,  0.2476, -0.3306, -0.1147, -0.3189, -0.2721,\n",
      "          0.1861,  0.3658, -0.1178, -0.2526,  0.4268, -0.3789,  0.2363,  0.3163,\n",
      "         -0.5742, -0.5057,  0.6257, -0.1252, -0.4093, -0.4726, -0.3989, -0.7954,\n",
      "          0.8309,  0.2792, -0.0184, -0.7120,  0.5947,  0.3068,  0.6849, -0.4791,\n",
      "         -0.3277, -0.6896,  0.3927,  0.5572, -0.3538, -0.3300, -0.3201,  0.3969,\n",
      "          0.4309,  0.2763,  0.0519, -0.2157,  0.2089, -0.3606,  0.5717,  0.3876,\n",
      "         -0.3085, -0.4781,  0.3728,  0.7356,  0.2805, -0.0760, -0.0306,  0.3238,\n",
      "          0.5568, -0.4982,  0.1902, -0.2717,  0.2298, -0.1054,  0.3472, -0.7049,\n",
      "         -0.3033, -0.1297, -0.4319,  0.2702, -0.3850, -0.8135, -0.5258,  0.2078,\n",
      "         -0.4755,  0.2760,  0.5246,  0.1623, -0.1243, -0.0386, -0.2085, -0.2480,\n",
      "         -0.1398,  0.1882, -0.1607, -0.2486,  0.6531, -0.0269,  0.2452,  0.2287,\n",
      "          0.4114,  0.3986, -0.3122,  0.3429,  0.1269,  0.3558,  0.5779, -0.1349,\n",
      "         -0.1263,  0.4484, -0.0808, -0.0540, -0.2884,  0.0060, -0.4632,  0.3299,\n",
      "         -0.8210,  0.1872, -0.5280, -0.3006, -0.2822,  0.1394, -0.3997,  0.3007,\n",
      "          0.0887,  0.2436, -0.3308, -0.3515,  0.2225, -0.0866,  0.2103,  0.6894,\n",
      "          0.0599, -0.3546,  0.3667, -0.1984, -0.6742,  0.1773,  0.3419,  0.4108,\n",
      "          0.0282, -0.2867, -0.6190, -0.6092, -0.1944, -0.6141, -0.0729, -0.6158,\n",
      "          0.2239, -0.2615, -0.4271, -0.0138,  0.3937,  0.1326,  0.4202,  0.6418,\n",
      "          0.6804,  0.0666,  0.4035,  0.5679, -0.2886, -0.1611,  0.3623, -0.3878,\n",
      "         -0.3601, -0.1200,  0.2705, -0.7192, -0.5186,  0.6713, -0.2218, -0.4345,\n",
      "          0.3989, -0.3472, -0.0871,  0.2099,  0.2555,  0.2056,  0.0648, -0.8244,\n",
      "          0.4136,  0.3455,  0.2007, -0.4165, -0.1019,  0.1231,  0.0832,  0.3096,\n",
      "          0.2223,  0.0402,  0.1016, -0.0559,  0.4966, -0.1094, -0.7510,  0.3040,\n",
      "          0.6841,  0.1068,  0.2348, -0.0136, -0.0613, -0.7650, -0.2315, -0.1548,\n",
      "         -0.5416,  0.2252,  0.1979, -0.1210,  0.3676,  0.1774,  0.4484, -0.7254]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(model(words_tensor, segments_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ff0d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat? esialgue läheme sellega...\n",
    "# overfittida väiksel korpusel\n",
    "\n",
    "# pos ja type mis ta nendega teeb? siis saab samal loogikal kanalid tekitadal\n",
    "# vocabid eraldi...\n",
    "\n",
    "# pärast saame timmida, nt. liitsõnad...\n",
    "\n",
    "# korpus võtta kuskilt cl ut ee korpused\n",
    "# estnltk tutorials corpus_processing\n",
    "\n",
    "# test eraldi kausta\n",
    "# ülesanne ennustada estnltk vormi\n",
    "# treenimisse sisse kirjutada vormi (ja lemma) ennustamine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac7f1985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.73 s\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Korpus\n",
    "\n",
    "from estnltk.corpus_processing.parse_enc import parse_enc_file_iterator\n",
    "\n",
    "input_file = \"estonian_nc17.vert\"\n",
    "n = 100 # Mitu teksti korpusesse lugeda\n",
    "korpus = []\n",
    "l = 0\n",
    "for text_obj in parse_enc_file_iterator(input_file):\n",
    "    korpus.append(text_obj.text)\n",
    "    if l > n:\n",
    "        break\n",
    "    l += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8a53ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from estnltk import Text\n",
    "tekst = Text(\" \".join(korpus)).tag_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "008daac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "laused = []\n",
    "for span in tekst.sentences:\n",
    "    laused.append(tekst.text[span.start:span.end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a73e5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train =  laused[:int(0.8*n)]\n",
    "test = laused[int(0.8*n):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6f47a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm(tensor):\n",
    "    rand = torch.rand(tensor[:, :, 0].shape)\n",
    "    mask_arr = (rand < 0.15) * (tensor[:, :, 0] > 5)\n",
    "    for i in range(tensor[:, :, 0].shape[0]):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        tensor[i, selection] = 4\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "433e44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained(\"tartuNLP/EstBERT\")\n",
    "tokenizer = bt(vocab_file = \"vocab.txt\", vocab_file_form = \"vocab_form.txt\")\n",
    "tokeniseeritud_lause = tokenizer(train[0:2], max_length = 64, padding = \"max_length\",\n",
    "                                     truncation = True, return_tensors = \"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "14b466ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2, 32476,  2733,     1,     1,  3937,  4462, 12535, 32476,  1773,\n",
       "         8285,   502,     1,    37,     1,     1, 14457,    11, 25063, 35501,\n",
       "            1,   179,   440, 18569,     1,    15,     3,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniseeritud_lause[\"input_ids\"][0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a749166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 1]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(tokeniseeritud_lause[\"input_ids\"][0] == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23235329",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(0,0)] + [(0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a8462",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "mask = []\n",
    "labels = []\n",
    "\n",
    "for lause in train:\n",
    "    tokeniseeritud_lause = tokenizer(lause, max_length = 64, padding = \"max_length\",\n",
    "                                     truncation = True, return_tensors = \"pt\")\n",
    "    labels.append([id[1] for id in tokeniseeritud_lause.input_ids])\n",
    "    mask.append(tokeniseeritud_lause.attention_mask)\n",
    "    input_ids.append(mlm(torch.tensor([tokeniseeritud_lause.input_ids]).detach().clone()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b94ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids)\n",
    "mask = torch.cat(mask)\n",
    "labels = torch.cat(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

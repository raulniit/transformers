{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe2b5b4",
   "metadata": {},
   "source": [
    "### Fail eeltreenimise protsessi testimiseks\n",
    "\n",
    "### Praeguseks on \"korpus\" kaustas .tsv failid tekitatud ehk script korpusest_tsv_andmete_loomine.py on jooksutatud. Kuna need on üle 2GB suured, ei saa neid GitHubi laadida. Näitena on kaustas \"test.tsv\" fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1090daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impordid\n",
    "import os\n",
    "import csv\n",
    "from datasets import load_dataset\n",
    "from src.transformers import BertTokenizer, BertForMaskedLM, BertConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c55d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['korpus\\\\test.tsv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"korpus\"\n",
    "input_files = [os.path.join(input_folder, filename) for filename in os.listdir(input_folder) if filename[-3:] == \"tsv\"]\n",
    "input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e142289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korpus\\test.tsv\n",
      "WARNING:builder.py:477: Using custom data configuration default-2b85cd7a5ac7a87f\n",
      "WARNING:builder.py:739: Found cached dataset csv (C:/Users/rauln/.cache/huggingface/datasets/csv/default-2b85cd7a5ac7a87f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a611c5c747c486181b9c58115fb3a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:fingerprint.py:328: Parameter 'function'=<function tokeniseeri_batch at 0x00000223A5256430> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a958d423a18747feafbd22d7fa469e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994b216455844ba7b5f8ad8612226b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Võtab iga batchi (tekstid korpusest, kus laused on eraldatud \\n sümboliga, 'text' : lause1 \\n lause2 \\n lause3)\n",
    "# Tokenieerib laused ja lisab batchile labelid\n",
    "# Väjund (x tähistab kõikide lausete arvu loetud batchi tekstides)\n",
    "# {'input_ids': x*128*2 tensor, 'attention_mask': x*128 tensor, 'token_type_ids': x*128 tensor, 'labels': x*128*2 tensor}\n",
    "def tokeniseeri_batch(batch):\n",
    "    batch_laused = [lause for para in batch[\"text\"] for lause in para.split(\"\\n\")]\n",
    "    tokeniseeritud = tokenizer(batch_laused, max_length = 128, padding = \"max_length\", truncation = True, return_tensors = \"pt\")\n",
    "    tokeniseeritud[\"labels\"] = tokeniseeritud[\"input_ids\"]\n",
    "    return tokeniseeritud\n",
    "\n",
    "# Tokeniseerija\n",
    "tokenizer = BertTokenizer(vocab_file = \"vocab_final.txt\", vocab_file_form = \"vocab_form.txt\", max_length = 128, padding = \"max_length\", truncation = True, return_tensors = \"pt\")\n",
    "\n",
    "for filename in input_files:\n",
    "    print(filename)\n",
    "    train_dataset = load_dataset(\"csv\", data_files={'train': filename}, names = [\"text\"], delimiter = \"\\t\")[\"train\"]\n",
    "    train_dataset_sisend = train_dataset.map(tokeniseeri_batch, batched=True, remove_columns=[\"text\"])\n",
    "    # train_dataset_sisend.set_format(type ='torch')\n",
    "    out_name = filename[:-4] + \".json\"\n",
    "    train_dataset_sisend.to_json(out_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a9b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(vocab_file = \"vocab_final.txt\", vocab_file_form = \"vocab_form.txt\", max_length = 128, \n",
    "                          padding = \"max_length\", truncation = True, return_tensors = \"pt\", mask_token = \"ˇMASKˇ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c83e61a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:builder.py:477: Using custom data configuration default-af143b59aca45e9b\n",
      "Downloading and preparing dataset json/default to C:/Users/rauln/.cache/huggingface/datasets/json/default-af143b59aca45e9b/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a03e111acc94c289c996143dc1f7e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc06c01d32b4447f89cd7f297768c807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/rauln/.cache/huggingface/datasets/json/default-af143b59aca45e9b/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc9198ba515424eb48139b619b998c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "treening_andmed = load_dataset(\"json\", data_files={'train': [\"korpus/test.json\"]})[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "479c6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=True,\n",
    "        mlm_probability=0.15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c05bba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "C:\\Users\\rauln\\Documents\\makatoo\\transformers\\src\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 62\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "  Number of trainable parameters = 160300708\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 03:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>14.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>13.169300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./train_results\\checkpoint-10\n",
      "Configuration saved in ./train_results\\checkpoint-10\\config.json\n",
      "Model weights saved in ./train_results\\checkpoint-10\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=13.590219879150391, metrics={'train_runtime': 264.3772, 'train_samples_per_second': 1.21, 'train_steps_per_second': 0.038, 'total_flos': 59133040005120.0, 'train_loss': 13.590219879150391, 'epoch': 5.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mudeli eeltreenimine\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    vocab_size_form = tokenizer.vocab_size_form,\n",
    "    tie_word_embeddings = False\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./train_results',\n",
    "    per_device_train_batch_size=32,\n",
    "    max_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    logging_dir='./train_logs',\n",
    "    logging_steps=5, \n",
    "    save_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset=treening_andmed\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25cda50",
   "metadata": {},
   "source": [
    "### Eeltreenitud mudeli laadimine ja hindamine/ennustamine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a55a86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file train_results\\checkpoint-10\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"hidden_size_form\": 48,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50005,\n",
      "  \"vocab_size_form\": 111\n",
      "}\n",
      "\n",
      "loading weights file train_results\\checkpoint-10\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at train_results\\checkpoint-10.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 62\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 13.126060485839844,\n",
       " 'eval_accuracy_lemma': 0.046511627906976744,\n",
       " 'eval_accuracy_vorm': 0.3488372093023256,\n",
       " 'eval_runtime': 15.7585,\n",
       " 'eval_samples_per_second': 3.934,\n",
       " 'eval_steps_per_second': 0.127}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    \n",
    "    indeksid = np.where(labels[:, :, 0].flatten() != -100)[0]\n",
    "    \n",
    "    labels_lemma = labels[:, :, 0].flatten()[indeksid]\n",
    "    labels_vorm = labels[:, :, 1].flatten()[indeksid]\n",
    "\n",
    "    pred_lemma = np.take(np.argmax(pred[0], axis = 2).flatten(), indeksid)\n",
    "    pred_vorm = np.take(np.argmax(pred[1], axis = 2).flatten(), indeksid)\n",
    "\n",
    "    accuracy_lemma = accuracy_score(y_true=labels_lemma, y_pred=pred_lemma)\n",
    "    accuracy_vorm = accuracy_score(y_true=labels_vorm, y_pred=pred_vorm)\n",
    "\n",
    "    return {\"accuracy_lemma\": accuracy_lemma, \"accuracy_vorm\": accuracy_vorm}\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(\"train_results\\checkpoint-10\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./eval_results',\n",
    "    per_device_eval_batch_size=32,\n",
    "    max_steps=10,\n",
    "    eval_steps = 10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args = training_args,\n",
    "    data_collator = data_collator,\n",
    "    eval_dataset = train_dataset_sisend,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70992ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 62\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_lemma: 0.03205128205128205\n",
      "accuracy_vorm: 0.3782051282051282\n"
     ]
    }
   ],
   "source": [
    "pred, labels, _ = trainer.predict(train_dataset_sisend)\n",
    "    \n",
    "indeksid = np.where(labels[:, :, 0].flatten() != -100)[0]\n",
    "    \n",
    "labels_lemma = labels[:, :, 0].flatten()[indeksid]\n",
    "labels_vorm = labels[:, :, 1].flatten()[indeksid]\n",
    "\n",
    "pred_lemma = np.take(np.argmax(pred[0], axis = 2).flatten(), indeksid)\n",
    "pred_vorm = np.take(np.argmax(pred[1], axis = 2).flatten(), indeksid)\n",
    "\n",
    "accuracy_lemma = accuracy_score(y_true=labels_lemma, y_pred=pred_lemma)\n",
    "accuracy_vorm = accuracy_score(y_true=labels_vorm, y_pred=pred_vorm)\n",
    "\n",
    "print(f\"accuracy_lemma: {accuracy_lemma}\")\n",
    "print(f\"accuracy_vorm: {accuracy_vorm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5138c243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dff7c173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13226,   495,     9,  2818,   481,  9807,  3001,  1323,    92,\n",
       "         303,   131,    52,    89,   418, 15864,   119,     6, 19765,\n",
       "        9874,  1324,   685,    77,   938,     6,   174,   161,  2051,\n",
       "           6,   113,  5376,   296,   917,   534,   596, 38325,  1571,\n",
       "         570,     5,     5,    10, 23473,    55,     6, 45401,    20,\n",
       "         385,    10,    55,  4297,    45,   647,    13,   263,   230,\n",
       "          15,  3068,   408, 23473,  2239,    75,  3068, 23473,   124,\n",
       "         796,   125,   407,   168,     6,   258,  1236,    21,   107,\n",
       "           6,   521,     7,    17,   899,   240,    19,    21,   682,\n",
       "           6,  1188,   321,    10,  1309,     5,  1551,    11,  1551,\n",
       "          12,  2316,    55,    32,  1224,  1224,   164,   473,   360,\n",
       "          14,    92,  2303,  7787, 11066,    25,   360,   840,    32,\n",
       "          24,  1083,   287,   305, 11264,    13,    31,    27,   266,\n",
       "           7,    84,   206,     5,    18,    46,     9,    16,  2291,\n",
       "        2176,   249,     5,  6350,   189,  2873,  2715,     7,     6,\n",
       "         187,    36,  2440,    41,  1490,     6,     7,    43,    14,\n",
       "          24,    64,    10,    15,    79,     6,    12,  1252,   138,\n",
       "        5699,    53,    12], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe102bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_vorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3445603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,  31,  31,  31,  56,  35,  35,  35,   5,  56,  29,   5,  56,\n",
       "        26,  35,   5,   5,  31,  33,  26,  31,  35,  37,   5,  31,   5,\n",
       "        26,   5,   5,  35,   5,  51,   5,  50,   6,  54,  36,   5,   5,\n",
       "         5,  36,   5,   5,  46,   5,  91,   5,   5,   6, 104,  31,  41,\n",
       "        26,  31,   5,  35,   5,  46,  36,  26,  31,  50,   5,  83,   5,\n",
       "        35,  90,   5,  29,   6,   5,   5,   5,   5,  54,   5,  31,  31,\n",
       "         5,   5,  26,   5,  56,  35,   5,  36,   5,  35,  31,  35,  72,\n",
       "        50,   5,   5,  35,  31,   5,  61,  31,   5,   5,  35,  31,  38,\n",
       "        29,  31,  31,   5,   5,  31,   5,  44,  29,  35,  56,   5,  54,\n",
       "        54,  31,   5,   5,  96,   5,  46,  31,  36,  51,  51,   5,  43,\n",
       "        56,  51,  56, 109,   5,   5,   5,  31,   5,  62,   5,  66,  50,\n",
       "         5,   5,  54,   5,   5,  38,   5,  72,  90,   5,   5,  56,  72],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_vorm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
